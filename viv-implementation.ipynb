{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Implement ViT (Vision Transformer) and Train On MNIST Dataset With YouTube Walkthrough\n---\n## Table of Contents\n* [Introduction](#introduction)\n* [YouTube Walkthrough](#youtube)\n* [Before You Begin](#begin)\n* [Data and Files](#datafiles)\n* [Implementation](#implementation)\n    * [Imports](#datafile)\n    * [Hyperparameter Definitions](#hyperparam)\n    * [ViT (Vision Transformer) Implementation](#vitimp)\n    * [EDA](#eda)\n    * [Split Datasets](#split)\n    * [Dataset and Dataloader Implementation](#dataobj)\n    * [Train Loop](#train)\n    * [Free-up Memory](#freememo)\n    * [Prediction Loop](#predict)\n    * [Submission File Preperation](#submit)\n* [Submission](#submit)\n* [References](#references)","metadata":{}},{"cell_type":"markdown","source":"## Introduction <a class=\"anchor\" id=\"introduction\"></a>\nThe aim of this work is to implement ViT (Vision Transformer) from scratch and train it on the MNIST dataset.\n\nViT (Vision Transformer) is from the paper - **An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale**: https://arxiv.org/abs/2010.11929\n\n---\nAny question and suggestion is appreciated. You can put them in the comments.","metadata":{}},{"cell_type":"markdown","source":"## YouTube Walkthrough <a class=\"anchor\" id=\"youtube\"></a>\nIf you want to see me implement and explain this notebook live, watch the YouTube video below.\n\n#### Kaggle Competition Walkthrough [NLP] - CommonLit Evaluate Student Summaries\n[<img src=\"https://i.imgur.com/pXKEE9g.png\" width=\"640\" height=\"360\"/>](https://www.youtube.com/watch?v=Vonyoz6Yt9c)","metadata":{}},{"cell_type":"markdown","source":"## Data and Files <a class=\"anchor\" id=\"datafiles\"></a>\n\nWe're given 3 files as inputs. *train.csv*, *test.csv* contains training and the test data, *sample_submission.csv* contains the submission format. Columns that files contain are:\n\n* `label` - the digit denoted in the image.\n* `pixelx` - id of the pixel and its correspoding value where x is between 0 and 783 inclusive.\n\n\n1. **train.csv**: contains the training data\n    * `label`\n    * `pixelx`\n  \n  \n2. **test.csv** contains the summaries training data\n    * `pixelx`","metadata":{}},{"cell_type":"markdown","source":"## Implementation <a class=\"anchor\" id=\"implementation\"></a>","metadata":{}},{"cell_type":"markdown","source":"### Imports <a class=\"anchor\" id=\"imports\"></a>","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport timeit\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-09-26T14:40:15.319872Z","iopub.execute_input":"2023-09-26T14:40:15.320131Z","iopub.status.idle":"2023-09-26T14:40:20.953313Z","shell.execute_reply.started":"2023-09-26T14:40:15.320106Z","shell.execute_reply":"2023-09-26T14:40:20.952266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter Definitions <a class=\"anchor\" id=\"hyperparam\"></a>","metadata":{}},{"cell_type":"code","source":"RANDOM_SEED = 42\nBATCH_SIZE = 512\nEPOCHS = 40\nLEARNING_RATE = 1e-4\nNUM_CLASSES = 10\nPATCH_SIZE = 4\nIMG_SIZE = 28\nIN_CHANNELS = 1\nNUM_HEADS = 8\nDROPOUT = 0.001\nHIDDEN_DIM = 768\nADAM_WEIGHT_DECAY = 0\nADAM_BETAS = (0.9, 0.999)\nACTIVATION=\"gelu\"\nNUM_ENCODERS = 4\nEMBED_DIM = (PATCH_SIZE ** 2) * IN_CHANNELS # 16\nNUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2 # 49\n\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed_all(RANDOM_SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2023-09-26T14:40:20.958652Z","iopub.execute_input":"2023-09-26T14:40:20.963686Z","iopub.status.idle":"2023-09-26T14:40:21.008339Z","shell.execute_reply.started":"2023-09-26T14:40:20.963650Z","shell.execute_reply":"2023-09-26T14:40:21.007489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ViT (Vision Transformer) Implementation <a class=\"anchor\" id=\"vitimp\"></a>","metadata":{}},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, embed_dim, patch_size, num_patches, dropout, in_channels):\n        super().__init__()\n        self.patcher = nn.Sequential(\n            nn.Conv2d(\n                in_channels=in_channels,\n                out_channels=embed_dim,\n                kernel_size=patch_size,\n                stride=patch_size,\n            ),                  \n            nn.Flatten(2))\n\n        self.cls_token = nn.Parameter(torch.randn(size=(1, in_channels, embed_dim)), requires_grad=True)\n        self.position_embeddings = nn.Parameter(torch.randn(size=(1, num_patches+1, embed_dim)), requires_grad=True)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x):\n        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n\n        x = self.patcher(x).permute(0, 2, 1)\n        x = torch.cat([cls_token, x], dim=1)\n        x = self.position_embeddings + x \n        x = self.dropout(x)\n        return x\n    \nmodel = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS).to(device)\nx = torch.randn(512, 1, 28, 28).to(device)\nprint(model(x).shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T14:44:18.471676Z","iopub.execute_input":"2023-09-26T14:44:18.472373Z","iopub.status.idle":"2023-09-26T14:44:23.087786Z","shell.execute_reply.started":"2023-09-26T14:44:18.472339Z","shell.execute_reply":"2023-09-26T14:44:23.086684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ViT(nn.Module):\n    def __init__(self, num_patches, img_size, num_classes, patch_size, embed_dim, num_encoders, num_heads, hidden_dim, dropout, activation, in_channels):\n        super().__init__()\n        self.embeddings_block = PatchEmbedding(embed_dim, patch_size, num_patches, dropout, in_channels)\n        \n        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, activation=activation, batch_first=True, norm_first=True)\n        self.encoder_blocks = nn.TransformerEncoder(encoder_layer, num_layers=num_encoders)\n\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(normalized_shape=embed_dim),\n            nn.Linear(in_features=embed_dim, out_features=num_classes)\n        )\n\n    def forward(self, x):\n        x = self.embeddings_block(x)\n        x = self.encoder_blocks(x)\n        x = self.mlp_head(x[:, 0, :])  # Apply MLP on the CLS token only\n        return x\n\nmodel = ViT(NUM_PATCHES, IMG_SIZE, NUM_CLASSES, PATCH_SIZE, EMBED_DIM, NUM_ENCODERS, NUM_HEADS, HIDDEN_DIM, DROPOUT, ACTIVATION, IN_CHANNELS).to(device)\nx = torch.randn(512, 1, 28, 28).to(device)\nprint(model(x).shape) # BATCH_SIZE X NUM_CLASSES","metadata":{"execution":{"iopub.status.busy":"2023-09-26T14:44:34.029018Z","iopub.execute_input":"2023-09-26T14:44:34.029391Z","iopub.status.idle":"2023-09-26T14:44:34.122786Z","shell.execute_reply.started":"2023-09-26T14:44:34.029359Z","shell.execute_reply":"2023-09-26T14:44:34.121796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EDA <a class=\"anchor\" id=\"eda\"></a>","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")\nsubmission_df = pd.read_csv(\"/kaggle/input/digit-recognizer/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-09-26T14:44:35.974369Z","iopub.execute_input":"2023-09-26T14:44:35.975856Z","iopub.status.idle":"2023-09-26T14:44:41.635006Z","shell.execute_reply.started":"2023-09-26T14:44:35.975813Z","shell.execute_reply":"2023-09-26T14:44:41.634012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T14:44:41.637083Z","iopub.execute_input":"2023-09-26T14:44:41.637438Z","iopub.status.idle":"2023-09-26T14:44:41.666402Z","shell.execute_reply.started":"2023-09-26T14:44:41.637406Z","shell.execute_reply":"2023-09-26T14:44:41.665539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T14:44:41.667773Z","iopub.execute_input":"2023-09-26T14:44:41.668330Z","iopub.status.idle":"2023-09-26T14:44:41.688494Z","shell.execute_reply.started":"2023-09-26T14:44:41.668298Z","shell.execute_reply":"2023-09-26T14:44:41.687400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T14:44:41.691457Z","iopub.execute_input":"2023-09-26T14:44:41.691818Z","iopub.status.idle":"2023-09-26T14:44:41.700019Z","shell.execute_reply.started":"2023-09-26T14:44:41.691782Z","shell.execute_reply":"2023-09-26T14:44:41.698824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split Datasets <a class=\"anchor\" id=\"split\"></a>","metadata":{}},{"cell_type":"code","source":"train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=RANDOM_SEED, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T14:44:41.701719Z","iopub.execute_input":"2023-09-26T14:44:41.702090Z","iopub.status.idle":"2023-09-26T14:44:41.905219Z","shell.execute_reply.started":"2023-09-26T14:44:41.702017Z","shell.execute_reply":"2023-09-26T14:44:41.904203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset and Dataloader Implementation <a class=\"anchor\" id=\"dataobj\"></a>","metadata":{}},{"cell_type":"code","source":"class MNISTTrainDataset(Dataset):\n    def __init__(self, images, labels, indicies):\n        self.images = images\n        self.labels = labels\n        self.indicies = indicies\n        self.transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.RandomRotation(15),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.5])\n        ])\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        image = self.images[idx].reshape((28, 28)).astype(np.uint8)\n        label = self.labels[idx]\n        index = self.indicies[idx]\n        image = self.transform(image)\n        \n        return {\"image\": image, \"label\": label, \"index\": index}\n    \nclass MNISTValDataset(Dataset):\n    def __init__(self, images, labels, indicies):\n        self.images = images\n        self.labels = labels\n        self.indicies = indicies\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.5])\n        ])\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        image = self.images[idx].reshape((28, 28)).astype(np.uint8)\n        label = self.labels[idx]\n        index = self.indicies[idx]\n        image = self.transform(image)\n        \n        return {\"image\": image, \"label\": label, \"index\": index}\n    \nclass MNISTSubmitDataset(Dataset):\n    def __init__(self, images, indicies):\n        self.images = images\n        self.indicies = indicies\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.5])\n        ])\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        image = self.images[idx].reshape((28, 28)).astype(np.uint8)\n        index = self.indicies[idx]\n        image = self.transform(image)\n        \n        return {\"image\": image, \"index\": index}","metadata":{"execution":{"iopub.status.busy":"2023-09-26T14:44:41.906834Z","iopub.execute_input":"2023-09-26T14:44:41.907233Z","iopub.status.idle":"2023-09-26T14:44:41.922234Z","shell.execute_reply.started":"2023-09-26T14:44:41.907200Z","shell.execute_reply":"2023-09-26T14:44:41.921270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nf, axarr = plt.subplots(1, 3)\n\ntrain_dataset = MNISTTrainDataset(train_df.iloc[:, 1:].values.astype(np.uint8), train_df.iloc[:, 0].values, train_df.index.values)\nprint(len(train_dataset))\nprint(train_dataset[0])\naxarr[0].imshow(train_dataset[0][\"image\"].squeeze(), cmap=\"gray\")\naxarr[0].set_title(\"Train Image\")\nprint(\"-\"*30)\n\nval_dataset = MNISTValDataset(val_df.iloc[:, 1:].values.astype(np.uint8), val_df.iloc[:, 0].values, val_df.index.values)\nprint(len(val_dataset))\nprint(val_dataset[0])\naxarr[1].imshow(val_dataset[0][\"image\"].squeeze(), cmap=\"gray\")\naxarr[1].set_title(\"Val Image\")\nprint(\"-\"*30)\n\ntest_dataset = MNISTSubmitDataset(test_df.values.astype(np.uint8), test_df.index.values)\nprint(len(test_dataset))\nprint(test_dataset[0])\naxarr[2].imshow(test_dataset[0][\"image\"].squeeze(), cmap=\"gray\")\naxarr[2].set_title(\"Test Image\")\nprint(\"-\"*30)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T14:44:41.923825Z","iopub.execute_input":"2023-09-26T14:44:41.924318Z","iopub.status.idle":"2023-09-26T14:44:42.501398Z","shell.execute_reply.started":"2023-09-26T14:44:41.924281Z","shell.execute_reply":"2023-09-26T14:44:42.500478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(dataset=train_dataset,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True)\n\nval_dataloader = DataLoader(dataset=val_dataset,\n                            batch_size=BATCH_SIZE,\n                            shuffle=True)\n\ntest_dataloader = DataLoader(dataset=test_dataset,\n                             batch_size=BATCH_SIZE,\n                             shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T14:44:42.888379Z","iopub.execute_input":"2023-09-26T14:44:42.888796Z","iopub.status.idle":"2023-09-26T14:44:42.896871Z","shell.execute_reply.started":"2023-09-26T14:44:42.888762Z","shell.execute_reply":"2023-09-26T14:44:42.895799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Loop <a class=\"anchor\" id=\"train\"></a>","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), betas=ADAM_BETAS, lr=LEARNING_RATE, weight_decay=ADAM_WEIGHT_DECAY)\n\nstart = timeit.default_timer()\nfor epoch in tqdm(range(EPOCHS), position=0, leave=True):\n    model.train()\n    train_labels = []\n    train_preds = []\n    train_running_loss = 0\n    for idx, img_label in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n        img = img_label[\"image\"].float().to(device)\n        label = img_label[\"label\"].type(torch.uint8).to(device)\n        y_pred = model(img)\n        y_pred_label = torch.argmax(y_pred, dim=1)\n\n        train_labels.extend(label.cpu().detach())\n        train_preds.extend(y_pred_label.cpu().detach())\n        \n        loss = criterion(y_pred, label)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_running_loss += loss.item()\n    train_loss = train_running_loss / (idx + 1)\n\n    model.eval()\n    val_labels = []\n    val_preds = []\n    val_running_loss = 0\n    with torch.no_grad():\n        for idx, img_label in enumerate(tqdm(val_dataloader, position=0, leave=True)):\n            img = img_label[\"image\"].float().to(device)\n            label = img_label[\"label\"].type(torch.uint8).to(device)         \n            y_pred = model(img)\n            y_pred_label = torch.argmax(y_pred, dim=1)\n            \n            val_labels.extend(label.cpu().detach())\n            val_preds.extend(y_pred_label.cpu().detach())\n            \n            loss = criterion(y_pred, label)\n            val_running_loss += loss.item()\n    val_loss = val_running_loss / (idx + 1)\n\n    print(\"-\"*30)\n    print(f\"Train Loss EPOCH {epoch+1}: {train_loss:.4f}\")\n    print(f\"Valid Loss EPOCH {epoch+1}: {val_loss:.4f}\")\n    print(f\"Train Accuracy EPOCH {epoch+1}: {sum(1 for x,y in zip(train_preds, train_labels) if x == y) / len(train_labels):.4f}\")\n    print(f\"Valid Accuracy EPOCH {epoch+1}: {sum(1 for x,y in zip(val_preds, val_labels) if x == y) / len(val_labels):.4f}\")\n    print(\"-\"*30)\n\nstop = timeit.default_timer()\nprint(f\"Training Time: {stop-start:.2f}s\")","metadata":{"execution":{"iopub.status.busy":"2023-09-26T14:44:51.880896Z","iopub.execute_input":"2023-09-26T14:44:51.881470Z","iopub.status.idle":"2023-09-26T14:56:27.518896Z","shell.execute_reply.started":"2023-09-26T14:44:51.881427Z","shell.execute_reply":"2023-09-26T14:56:27.517839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Free-up Memory <a class=\"anchor\" id=\"freememo\"></a>","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T14:56:27.520895Z","iopub.execute_input":"2023-09-26T14:56:27.521475Z","iopub.status.idle":"2023-09-26T14:56:27.577032Z","shell.execute_reply.started":"2023-09-26T14:56:27.521442Z","shell.execute_reply":"2023-09-26T14:56:27.576023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction Loop <a class=\"anchor\" id=\"predict\"></a>","metadata":{}},{"cell_type":"code","source":"labels = []\nids = []\nimgs = []\nmodel.eval()\nwith torch.no_grad():\n    for idx, sample in enumerate(tqdm(test_dataloader, position=0, leave=True)):\n        img = sample[\"image\"].to(device)\n        ids.extend([int(i)+1 for i in sample[\"index\"]])\n        \n        outputs = model(img)\n        \n        imgs.extend(img.detach().cpu())\n        labels.extend([int(i) for i in torch.argmax(outputs, dim=1)])","metadata":{"execution":{"iopub.status.busy":"2023-09-26T15:27:53.530890Z","iopub.execute_input":"2023-09-26T15:27:53.531252Z","iopub.status.idle":"2023-09-26T15:27:58.155874Z","shell.execute_reply.started":"2023-09-26T15:27:53.531223Z","shell.execute_reply":"2023-09-26T15:27:58.154902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nf, axarr = plt.subplots(2, 3)\ncounter = 0\nfor i in range(2):\n    for j in range(3):\n        axarr[i][j].imshow(imgs[counter].squeeze(), cmap=\"gray\")\n        axarr[i][j].set_title(f\"Predicted {labels[counter]}\")\n        counter += 1","metadata":{"execution":{"iopub.status.busy":"2023-09-26T15:42:29.856995Z","iopub.execute_input":"2023-09-26T15:42:29.857582Z","iopub.status.idle":"2023-09-26T15:42:31.213526Z","shell.execute_reply.started":"2023-09-26T15:42:29.857521Z","shell.execute_reply":"2023-09-26T15:42:31.212512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission File Preperartion <a class=\"anchor\" id=\"submit\"></a>","metadata":{}},{"cell_type":"code","source":"submission_df = pd.DataFrame(list(zip(ids, labels)),\n               columns =[\"ImageId\", \"Label\"])\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T15:26:49.300211Z","iopub.execute_input":"2023-09-26T15:26:49.300600Z","iopub.status.idle":"2023-09-26T15:26:49.399094Z","shell.execute_reply.started":"2023-09-26T15:26:49.300546Z","shell.execute_reply":"2023-09-26T15:26:49.397973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission <a class=\"anchor\" id=\"submission\"></a>\n\nIf your notebook runs smoothly you can go to the right panel and click submit. Congratulations :)     \nAny question and suggestion is appreciated. You can put them in the comments.","metadata":{}},{"cell_type":"markdown","source":"## References <a class=\"anchor\" id=\"references\"></a>\n\n* https://www.kaggle.com/code/ignazio/vision-transformers-in-pytorch-for-mnist-handwritt\n* https://www.kaggle.com/code/taranmarley/vitransformer-step-by-step","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/8gNhPcv.jpg\"/>","metadata":{}}]}